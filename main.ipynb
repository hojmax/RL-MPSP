{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReplayBuffer import ReplayBuffer\n",
    "from DQN_Solver import DQN_Solver\n",
    "from env import MPSPEnv\n",
    "from DQN import DQN\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.ipynb'\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Env\n",
    "    'ROWS': 3,\n",
    "    'COLUMNS': 3,\n",
    "    'N_PORTS': 5,\n",
    "    # Training\n",
    "    'EPISODES': 2000,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'ADAM_EPSILON': 0.01,\n",
    "    'MEM_SIZE': 10000,\n",
    "    'BATCH_SIZE': 100,\n",
    "    'GAMMA': 0.95,\n",
    "    'EXPLORATION_MAX': 1.0,\n",
    "    'EXPLORATION_DECAY': 0.999,\n",
    "    'EXPLORATION_MIN': 0.005,\n",
    "    'EVAL_EPISODES': 50,\n",
    "    'MAX_EPISODE_STEPS': 200,\n",
    "    'TARGET_UPDATE_FREQ': 500,\n",
    "    'GRADIENT_CLIP': 5,\n",
    "    # Model\n",
    "    'HIDDEN_SIZE': 256,\n",
    "    'N_LAYERS': 4,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MPSPEnv(\n",
    "    config['ROWS'],\n",
    "    config['COLUMNS'],\n",
    "    config['N_PORTS']\n",
    ")\n",
    "# We flatten the observation space\n",
    "config['OBSERVATION_SPACE'] = (\n",
    "    np.prod(env.observation_space[0].shape) +\n",
    "    np.prod(env.observation_space[1].shape)\n",
    ")\n",
    "config['ACTION_SPACE'] = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Q-learning\",\n",
    "    entity=\"rl-msps\",\n",
    "    name=f\"{config['ROWS']}x{config['COLUMNS']}_{config['N_PORTS']}-ports\",\n",
    "    config=config,\n",
    "    tags=[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplayBuffer = ReplayBuffer(\n",
    "    mem_size=config['MEM_SIZE'],\n",
    "    observation_space=config['OBSERVATION_SPACE'],\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "current_DQN = DQN(\n",
    "    input_size=config['OBSERVATION_SPACE'],\n",
    "    output_size=config['ACTION_SPACE'],\n",
    "    hidden_size=config['HIDDEN_SIZE'],\n",
    "    n_layers=config['N_LAYERS'],\n",
    "    learning_rate=config['LEARNING_RATE'],\n",
    "    adam_epsilon=config['ADAM_EPSILON']\n",
    ")\n",
    "target_DQN = DQN(\n",
    "    input_size=config['OBSERVATION_SPACE'],\n",
    "    output_size=config['ACTION_SPACE'],\n",
    "    hidden_size=config['HIDDEN_SIZE'],\n",
    "    n_layers=config['N_LAYERS'],\n",
    "    learning_rate=config['LEARNING_RATE'],\n",
    "    adam_epsilon=config['ADAM_EPSILON']\n",
    ")\n",
    "agent = DQN_Solver(\n",
    "    ReplayBuffer=ReplayBuffer,\n",
    "    DQN=current_DQN,\n",
    "    target_DQN=target_DQN,\n",
    "    batch_size=config['BATCH_SIZE'],\n",
    "    exploration_max=config['EXPLORATION_MAX'],\n",
    "    gamma=config['GAMMA'],\n",
    "    exploration_decay=config['EXPLORATION_DECAY'],\n",
    "    exploration_min=config['EXPLORATION_MIN'],\n",
    "    target_update_freq=config['TARGET_UPDATE_FREQ'],\n",
    "    gradient_clip=config['GRADIENT_CLIP']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()\n",
    "\n",
    "for i in range(1, config['EPISODES']):\n",
    "    state, info = env.reset()\n",
    "    state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "    sum_reward = 0\n",
    "    sum_loss = 0\n",
    "    iter = 0\n",
    "\n",
    "    while iter < config['MAX_EPISODE_STEPS']:\n",
    "        action, _ = agent.choose_action(state, info['mask'], env)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        sum_loss += agent.learn()\n",
    "        state = state_\n",
    "        sum_reward += reward\n",
    "        iter += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    wandb.log({\n",
    "        \"Sum Episode Reward\": sum_reward,\n",
    "        \"Avg. Episode Loss\": sum_loss / iter,\n",
    "        \"Exploration Rate\": agent.exploration_rate\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(current_DQN.state_dict(), os.path.join(wandb.run.dir, \"dqn.pt\"))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59669bb1168edc622bb4dae0f982b2741a2a7b3cacb44a90a5e1c2622e59ac77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
