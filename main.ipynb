{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojmax\u001b[0m (\u001b[33mrl-msps\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ReplayBuffer import ReplayBuffer\n",
    "from DQN_Solver import DQN_Solver\n",
    "from env import MPSPEnv\n",
    "from DQN import DQN\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.ipynb'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Env\n",
    "    'ROWS': 3,\n",
    "    'COLUMNS': 3,\n",
    "    'N_PORTS': 5,\n",
    "    # Training\n",
    "    'EPISODES': 1000,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'MEM_SIZE': 10000,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'GAMMA': 0.95,\n",
    "    'EXPLORATION_MAX': 1.0,\n",
    "    'EXPLORATION_DECAY': 0.999999,\n",
    "    'EXPLORATION_MIN': 0.001,\n",
    "    'EVAL_EPISODES': 50,\n",
    "    'MAX_EPISODE_STEPS': 200,\n",
    "    # Model\n",
    "    'HIDDEN_SIZE': 128,\n",
    "    'N_LAYERS': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MPSPEnv(\n",
    "    config['ROWS'],\n",
    "    config['COLUMNS'],\n",
    "    config['N_PORTS']\n",
    ")\n",
    "# We flatten the observation space\n",
    "config['OBSERVATION_SPACE'] = (\n",
    "    np.prod(env.observation_space[0].shape) +\n",
    "    np.prod(env.observation_space[1].shape)\n",
    ")\n",
    "config['ACTION_SPACE'] = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/axelhojmark/Desktop/RL-MPSP/wandb/run-20221202_195054-2hv4v5w7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rl-msps/Q-learning/runs/2hv4v5w7\" target=\"_blank\">3x3_5-ports</a></strong> to <a href=\"https://wandb.ai/rl-msps/Q-learning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rl-msps/Q-learning/runs/2hv4v5w7?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7faca9e28b80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"Q-learning\",\n",
    "    entity=\"rl-msps\",\n",
    "    name=f\"{config['ROWS']}x{config['COLUMNS']}_{config['N_PORTS']}-ports\",\n",
    "    config=config,\n",
    "    tags=[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplayBuffer = ReplayBuffer(\n",
    "    mem_size=config['MEM_SIZE'],\n",
    "    observation_space=config['OBSERVATION_SPACE'],\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "DQN = DQN(\n",
    "    input_size=config['OBSERVATION_SPACE'],\n",
    "    output_size=config['ACTION_SPACE'],\n",
    "    hidden_size=config['HIDDEN_SIZE'],\n",
    "    n_layers=config['N_LAYERS'],\n",
    "    learning_rate=config['LEARNING_RATE']\n",
    ")\n",
    "agent = DQN_Solver(\n",
    "    ReplayBuffer=ReplayBuffer,\n",
    "    DQN=DQN,\n",
    "    batch_size=config['BATCH_SIZE'],\n",
    "    exploration_max=config['EXPLORATION_MAX'],\n",
    "    gamma=config['GAMMA'],\n",
    "    exploration_decay=config['EXPLORATION_DECAY'],\n",
    "    exploration_min=config['EXPLORATION_MIN']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0., -1., -1., -1.,  0.,  0.,\n",
      "         0., -1.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0., -1., -1.,\n",
      "         0., -1., -1.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0., -1.,\n",
      "         0.,  0., -1., -1.,  0.,  0., -1.,  0., -1., -1.,  0., -1.,  0., -1.,\n",
      "         0.,  0., -1., -1., -1., -1.,  0.,  0.], device='mps:0')\n",
      "Target:\n",
      "tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0., -1., -1., -1.,  0.,  0.,\n",
      "         0., -1.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0., -1.,  0., -1., -1.,\n",
      "         0., -1., -1.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0., -1.,\n",
      "         0.,  0., -1., -1.,  0.,  0., -1.,  0., -1., -1.,  0., -1.,  0., -1.,\n",
      "         0.,  0., -1., -1., -1., -1.,  0.,  0.], device='mps:0')\n",
      "Predicted:\n",
      "tensor([-1.1506e-03, -2.7169e-03, -9.8151e-01,  2.0850e-02, -1.3024e-03,\n",
      "        -2.4603e-03, -8.3304e-03, -8.8881e-01, -6.0909e-03, -9.7924e-01,\n",
      "        -1.0394e+00, -1.0826e+00, -6.1674e-03, -1.1971e-02, -3.6280e-03,\n",
      "        -9.5361e-01, -1.3397e-02, -9.5931e-01, -3.8598e-03, -9.3078e-01,\n",
      "         2.2772e-03, -9.7140e-03,  9.8505e-03, -1.5364e-03, -1.0032e+00,\n",
      "        -9.1366e-03, -9.8627e-01, -9.7959e-01, -4.2925e-03, -1.0336e+00,\n",
      "        -9.7399e-01, -3.0367e-03, -1.0123e+00, -1.0209e-04, -3.8727e-03,\n",
      "        -6.0042e-03, -9.7510e-01, -5.7258e-03, -5.1429e-03, -1.0207e+00,\n",
      "        -7.0697e-03, -1.0176e+00, -6.2797e-03, -5.2292e-03, -8.8982e-01,\n",
      "        -9.2625e-01, -8.8018e-04, -6.2316e-03, -9.2333e-01, -1.0522e-02,\n",
      "        -1.0724e+00, -9.7063e-01,  2.7830e-03, -9.7438e-01,  1.4178e-03,\n",
      "        -9.8399e-01, -5.5799e-03, -3.3433e-03, -1.0938e+00, -9.3672e-01,\n",
      "        -9.7345e-01, -9.0604e-01, -6.0675e-03, -3.7117e-03], device='mps:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "agent.train()\n",
    "\n",
    "for i in range(1, config['EPISODES']):\n",
    "    state, info = env.reset()\n",
    "    state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "    sum_reward = 0\n",
    "    sum_loss = 0\n",
    "    iter = 0\n",
    "\n",
    "    while iter < config['MAX_EPISODE_STEPS']:\n",
    "        action, _ = agent.choose_action(state, info['mask'], env)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        sum_loss += agent.learn(iter == 0)\n",
    "        state = state_\n",
    "        sum_reward += reward\n",
    "        iter += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    wandb.log({\n",
    "        \"Sum Episode Reward\": sum_reward,\n",
    "        \"Avg. Episode Loss\": sum_loss / iter,\n",
    "        \"Exploration Rate\": agent.exploration_rate\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eval()\n",
    "sum_reward = 0\n",
    "\n",
    "for i in range(1, config['EVAL_EPISODES']):\n",
    "    state, info = env.reset()\n",
    "    state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "\n",
    "    while True:\n",
    "        action, _ = agent.choose_action(state, info['mask'], env)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        state = state_\n",
    "        sum_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "wandb.summary['Avg. Eval Episode Reward'] = (\n",
    "    sum_reward / config['EVAL_EPISODES']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DQN.state_dict(), os.path.join(wandb.run.dir, \"dqn.pt\"))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59669bb1168edc622bb4dae0f982b2741a2a7b3cacb44a90a5e1c2622e59ac77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
