{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojmax\u001b[0m (\u001b[33mrl-msps\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ReplayBuffer import ReplayBuffer\n",
    "from DQN_Solver import DQN_Solver\n",
    "from env import MPSPEnv\n",
    "from DQN import DQN\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.ipynb'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Env\n",
    "    'ROWS': 3,\n",
    "    'COLUMNS': 3,\n",
    "    'N_PORTS': 4,\n",
    "    # Training\n",
    "    'EPISODES': 100,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'MEM_SIZE': 10000,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'GAMMA': 0.95,\n",
    "    'EXPLORATION_MAX': 1.0,\n",
    "    'EXPLORATION_DECAY': 0.999,\n",
    "    'EXPLORATION_MIN': 0.001,\n",
    "    'EVAL_EPISODES': 50,\n",
    "    # Model\n",
    "    'HIDDEN_SIZE_1': 100,\n",
    "    'HIDDEN_SIZE_2': 100,\n",
    "    'HIDDEN_SIZE_3': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MPSPEnv(\n",
    "    config['ROWS'],\n",
    "    config['COLUMNS'],\n",
    "    config['N_PORTS']\n",
    ")\n",
    "# We flatten the observation space\n",
    "config['OBSERVATION_SPACE'] = (\n",
    "    np.prod(env.observation_space[0].shape) +\n",
    "    np.prod(env.observation_space[1].shape)\n",
    ")\n",
    "config['ACTION_SPACE'] = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/axelhojmark/Desktop/RL-MPSP/wandb/run-20221201_213341-21u2cb6k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rl-msps/Q-learning/runs/21u2cb6k\" target=\"_blank\">3x3_4-ports</a></strong> to <a href=\"https://wandb.ai/rl-msps/Q-learning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rl-msps/Q-learning/runs/21u2cb6k?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8a49d169a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"Q-learning\",\n",
    "    entity=\"rl-msps\",\n",
    "    name=f\"{config['ROWS']}x{config['COLUMNS']}_{config['N_PORTS']}-ports\",\n",
    "    config=config,\n",
    "    tags=[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplayBuffer = ReplayBuffer(\n",
    "    mem_size=config['MEM_SIZE'],\n",
    "    observation_space=config['OBSERVATION_SPACE'],\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "DQN = DQN(\n",
    "    input_size=config['OBSERVATION_SPACE'],\n",
    "    output_size=config['ACTION_SPACE'],\n",
    "    hidden_size_1=config['HIDDEN_SIZE_1'],\n",
    "    hidden_size_2=config['HIDDEN_SIZE_2'],\n",
    "    hidden_size_3=config['HIDDEN_SIZE_3'],\n",
    "    learning_rate=config['LEARNING_RATE']\n",
    ")\n",
    "agent = DQN_Solver(\n",
    "    ReplayBuffer=ReplayBuffer,\n",
    "    DQN=DQN,\n",
    "    batch_size=config['BATCH_SIZE'],\n",
    "    exploration_max=config['EXPLORATION_MAX'],\n",
    "    gamma=config['GAMMA'],\n",
    "    exploration_decay=config['EXPLORATION_DECAY'],\n",
    "    exploration_min=config['EXPLORATION_MIN']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()\n",
    "\n",
    "for i in range(1, config['EPISODES']):\n",
    "    state, info = env.reset()\n",
    "    state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "    score = 0\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        action = agent.choose_action(state, info['mask'], env)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        sum_loss += agent.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "        count += 1\n",
    "\n",
    "        if done:\n",
    "            wandb.log({\n",
    "                \"Episode Reward\": score,\n",
    "                \"Average Episode Loss\": sum_loss / count,\n",
    "                \"Exploration Rate\": agent.exploration_rate\n",
    "            })\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j8/qdr4bmbd6vv8nydcr40cfdhw0000gn/T/ipykernel_62666/238381355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mstate_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/RL-MPSP/env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/RL-MPSP/env.py\u001b[0m in \u001b[0;36m_add_container\u001b[0;34m(self, i, j)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Find last destination container\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_last_destination_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mcontainer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Update state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.eval()\n",
    "sum_reward = 0\n",
    "\n",
    "for i in range(1, config['EVAL_EPISODES']):\n",
    "    state, info = env.reset()\n",
    "    state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "\n",
    "    while True:\n",
    "        action = agent.choose_action(state, info['mask'], env)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        state = state_\n",
    "        sum_reward += reward\n",
    "\n",
    "wandb.summary['avg_eval_reward'] = sum_reward / config['EVAL_EPISODES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b299c39a89934f43b80a6941f0222943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.114 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.008816…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Episode Loss</td><td>█▁▁▂▁▂▁▂▂▂▂▃▂▃▂▂▂▂▂▂▁▂▁▂▂▃▃▂▂▁▁▃▂▂▂▂▁▁▂▁</td></tr><tr><td>Episode Reward</td><td>██▃▆███▆█▃█▆▃▆███████████▁██▆▆█▆█▆█▆███▆</td></tr><tr><td>Exploration Rate</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Episode Loss</td><td>0.03411</td></tr><tr><td>Episode Reward</td><td>0</td></tr><tr><td>Exploration Rate</td><td>0.001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">3x3_4-ports</strong>: <a href=\"https://wandb.ai/rl-msps/Q-learning/runs/mb9n5roz\" target=\"_blank\">https://wandb.ai/rl-msps/Q-learning/runs/mb9n5roz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221201_211922-mb9n5roz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(DQN.state_dict(), os.path.join(wandb.run.dir, \"dqn.pt\"))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59669bb1168edc622bb4dae0f982b2741a2a7b3cacb44a90a5e1c2622e59ac77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
