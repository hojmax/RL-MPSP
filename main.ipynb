{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojmax\u001b[0m (\u001b[33mrl-msps\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ReplayBuffer import ReplayBuffer\n",
    "from DQN_Solver import DQN_Solver\n",
    "from env import MPSPEnv\n",
    "from DQN import DQN\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'main.ipynb'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Env\n",
    "    'ROWS': 3,\n",
    "    'COLUMNS': 3,\n",
    "    'N_PORTS': 5,\n",
    "    # Training\n",
    "    'EPISODES': 1000,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'MEM_SIZE': 10000,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'GAMMA': 0.95,\n",
    "    'EXPLORATION_MAX': 1.0,\n",
    "    'EXPLORATION_DECAY': 0.99,\n",
    "    'EXPLORATION_MIN': 0.001,\n",
    "    'EVAL_EPISODES': 50,\n",
    "    'MAX_EPISODE_STEPS': 200,\n",
    "    # Model\n",
    "    'HIDDEN_SIZE': 64,\n",
    "    'N_LAYERS': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# MPSPEnv(\n",
    "#     config['ROWS'],\n",
    "#     config['COLUMNS'],\n",
    "#     config['N_PORTS']\n",
    "# )\n",
    "# We flatten the observation space\n",
    "config['OBSERVATION_SPACE'] = env.observation_space.shape[0]\n",
    "#  (\n",
    "#     np.prod(env.observation_space[0].shape) +\n",
    "#     np.prod(env.observation_space[1].shape)\n",
    "# )\n",
    "config['ACTION_SPACE'] = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Q-learning\",\n",
    "    entity=\"rl-msps\",\n",
    "    name=\"cartpole\",\n",
    "    # name=f\"{config['ROWS']}x{config['COLUMNS']}_{config['N_PORTS']}-ports\",\n",
    "    config=config,\n",
    "    tags=[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplayBuffer = ReplayBuffer(\n",
    "    mem_size=config['MEM_SIZE'],\n",
    "    observation_space=config['OBSERVATION_SPACE'],\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "DQN = DQN(\n",
    "    input_size=config['OBSERVATION_SPACE'],\n",
    "    output_size=config['ACTION_SPACE'],\n",
    "    hidden_size=config['HIDDEN_SIZE'],\n",
    "    n_layers=config['N_LAYERS'],\n",
    "    learning_rate=config['LEARNING_RATE']\n",
    ")\n",
    "agent = DQN_Solver(\n",
    "    ReplayBuffer=ReplayBuffer,\n",
    "    DQN=DQN,\n",
    "    batch_size=config['BATCH_SIZE'],\n",
    "    exploration_max=config['EXPLORATION_MAX'],\n",
    "    gamma=config['GAMMA'],\n",
    "    exploration_decay=config['EXPLORATION_DECAY'],\n",
    "    exploration_min=config['EXPLORATION_MIN']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()\n",
    "\n",
    "for i in range(1, config['EPISODES']):\n",
    "    state, info = env.reset()\n",
    "    # state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "    sum_reward = 0\n",
    "    sum_loss = 0\n",
    "    iter = 0\n",
    "\n",
    "    while iter < config['MAX_EPISODE_STEPS']:\n",
    "        action, _ = agent.choose_action(state, np.ones(2, dtype=np.int8) , env) #info['mask']\n",
    "        state_, reward, done, _, info = env.step(action)\n",
    "        # state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        sum_loss += agent.learn()\n",
    "        state = state_\n",
    "        sum_reward += reward\n",
    "        iter += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    wandb.log({\n",
    "        \"Sum Episode Reward\": sum_reward,\n",
    "        \"Avg. Episode Loss\": sum_loss / iter,\n",
    "        \"Exploration Rate\": agent.exploration_rate\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eval()\n",
    "sum_reward = 0\n",
    "\n",
    "for i in range(1, config['EVAL_EPISODES']):\n",
    "    state, info = env.reset()\n",
    "    state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "\n",
    "    while True:\n",
    "        action, _ = agent.choose_action(state, info['mask'], env)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        state = state_\n",
    "        sum_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "wandb.summary['Avg. Eval Episode Reward'] = (\n",
    "    sum_reward / config['EVAL_EPISODES']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DQN.state_dict(), os.path.join(wandb.run.dir, \"dqn.pt\"))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59669bb1168edc622bb4dae0f982b2741a2a7b3cacb44a90a5e1c2622e59ac77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
