{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from env import MPSPEnv\n",
    "\n",
    "env = MPSPEnv(3, 3, 3)\n",
    "observation_space = env.observation_space[0].shape[0]**2 + \\\n",
    "    env.observation_space[1].shape[0]**2\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# observation_space = env.observation_space.shape[0]\n",
    "\n",
    "action_space = env.action_space.n\n",
    "\n",
    "EPISODES = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "MEM_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.999\n",
    "EXPLORATION_MIN = 0.001\n",
    "\n",
    "FC1_DIMS = 1024\n",
    "FC2_DIMS = 512\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_shape, FC1_DIMS)\n",
    "        self.fc2 = nn.Linear(FC1_DIMS, FC2_DIMS)\n",
    "        self.fc3 = nn.Linear(FC2_DIMS, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.mem_count = 0\n",
    "\n",
    "        self.states = np.zeros((MEM_SIZE, observation_space), dtype=np.int64)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.states_ = np.zeros((MEM_SIZE, observation_space), dtype=np.int64)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=bool)\n",
    "\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        mem_index = self.mem_count % MEM_SIZE\n",
    "\n",
    "        self.states[mem_index] = state\n",
    "        self.actions[mem_index] = action\n",
    "        self.rewards[mem_index] = reward\n",
    "        self.states_[mem_index] = state_\n",
    "        self.dones[mem_index] = 1 - done\n",
    "\n",
    "        self.mem_count += 1\n",
    "\n",
    "    def sample(self):\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
    "\n",
    "        states = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        dones = self.dones[batch_indices]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "\n",
    "class DQN_Solver:\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network()\n",
    "\n",
    "    def choose_action(self, observation, mask):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return env.action_space.sample(mask)\n",
    "\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.to(DEVICE)\n",
    "        state = state.unsqueeze(0)\n",
    "        q_values = self.network(state).detach()\n",
    "        q_max = q_values.abs().max()\n",
    "        masked_argmax = (\n",
    "            q_values - 2 * q_max * (1 - mask)\n",
    "        ).argmax()\n",
    "        return masked_argmax.item()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_count < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.memory.sample()\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(DEVICE)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(DEVICE)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32).to(DEVICE)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(DEVICE)\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        q_values = self.network(states)\n",
    "        next_q_values = self.network(states_)\n",
    "\n",
    "        predicted_value_of_now = q_values[batch_indices, actions]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "\n",
    "        q_target = rewards + GAMMA * predicted_value_of_future * dones\n",
    "\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN_Solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Average Reward -432.0 Best Reward 0 Last Reward -13 Epsilon 1.0\n",
      "Episode 2 Average Reward -236.5 Best Reward 0 Last Reward -41 Epsilon 0.9360999518731578\n",
      "Episode 3 Average Reward -169.0 Best Reward 0 Last Reward -34 Epsilon 0.862367254825433\n",
      "Episode 4 Average Reward -131.25 Best Reward 0 Last Reward -18 Epsilon 0.8235779600286178\n",
      "Episode 5 Average Reward -105.0 Best Reward 0 Last Reward 0 Epsilon 0.818648829478636\n",
      "Episode 6 Average Reward -88.66666666666667 Best Reward 0 Last Reward -7 Epsilon 0.8024304668606914\n",
      "Episode 7 Average Reward -79.85714285714286 Best Reward 0 Last Reward -27 Epsilon 0.7534131012276413\n",
      "Episode 8 Average Reward -70.0 Best Reward 0 Last Reward -1 Epsilon 0.7474068498462175\n",
      "Episode 9 Average Reward -62.22222222222222 Best Reward 0 Last Reward 0 Epsilon 0.7444217038990517\n",
      "Episode 10 Average Reward -56.2 Best Reward 0 Last Reward -2 Epsilon 0.7340672721936974\n",
      "Episode 11 Average Reward -52.90909090909091 Best Reward 0 Last Reward -20 Epsilon 0.6989478686545415\n",
      "Episode 12 Average Reward -48.583333333333336 Best Reward 0 Last Reward -1 Epsilon 0.6933758171534341\n",
      "Episode 13 Average Reward -45.15384615384615 Best Reward 0 Last Reward -4 Epsilon 0.6823646221455009\n",
      "Episode 14 Average Reward -44.214285714285715 Best Reward 0 Last Reward -32 Epsilon 0.6311383701174348\n",
      "Episode 15 Average Reward -41.4 Best Reward 0 Last Reward -2 Epsilon 0.623606226269926\n",
      "Episode 16 Average Reward -39.625 Best Reward 0 Last Reward -13 Epsilon 0.5973466554009469\n",
      "Episode 17 Average Reward -37.470588235294116 Best Reward 0 Last Reward -3 Epsilon 0.5884489059896089\n",
      "Episode 18 Average Reward -35.44444444444444 Best Reward 0 Last Reward -1 Epsilon 0.5837577583990794\n",
      "Episode 19 Average Reward -33.578947368421055 Best Reward 0 Last Reward 0 Epsilon 0.5802639565486586\n",
      "Episode 20 Average Reward -31.9 Best Reward 0 Last Reward 0 Epsilon 0.577368433605742\n",
      "Episode 21 Average Reward -30.61904761904762 Best Reward 0 Last Reward -5 Epsilon 0.5647988152354793\n",
      "Episode 22 Average Reward -29.363636363636363 Best Reward 0 Last Reward -3 Epsilon 0.5552736652928869\n",
      "Episode 23 Average Reward -28.17391304347826 Best Reward 0 Last Reward -2 Epsilon 0.5464556095429605\n",
      "Episode 24 Average Reward -27.125 Best Reward 0 Last Reward -3 Epsilon 0.5393941542601555\n",
      "Episode 25 Average Reward -26.16 Best Reward 0 Last Reward -3 Epsilon 0.5292373811410898\n",
      "Episode 26 Average Reward -25.23076923076923 Best Reward 0 Last Reward -2 Epsilon 0.5234447908547367\n",
      "Episode 27 Average Reward -24.333333333333332 Best Reward 0 Last Reward -1 Epsilon 0.5182338352581944\n",
      "Episode 28 Average Reward -23.642857142857142 Best Reward 0 Last Reward -5 Epsilon 0.5069516085956246\n",
      "Episode 29 Average Reward -23.379310344827587 Best Reward 0 Last Reward -16 Epsilon 0.48560427251675453\n",
      "Episode 30 Average Reward -23.166666666666668 Best Reward 0 Last Reward -17 Epsilon 0.46237188898344517\n",
      "Episode 31 Average Reward -22.70967741935484 Best Reward 0 Last Reward -9 Epsilon 0.4478030481625413\n",
      "Episode 32 Average Reward -22.0 Best Reward 0 Last Reward 0 Epsilon 0.44601452099741573\n",
      "Episode 33 Average Reward -21.424242424242426 Best Reward 0 Last Reward -3 Epsilon 0.43849262988420123\n",
      "Episode 34 Average Reward -20.852941176470587 Best Reward 0 Last Reward -2 Epsilon 0.43066649544671043\n",
      "Episode 35 Average Reward -20.285714285714285 Best Reward 0 Last Reward -1 Epsilon 0.42595277973599555\n",
      "Episode 36 Average Reward -19.77777777777778 Best Reward 0 Last Reward -2 Epsilon 0.41835045419835276\n",
      "Episode 37 Average Reward -19.27027027027027 Best Reward 0 Last Reward -1 Epsilon 0.41377153958718943\n",
      "Episode 38 Average Reward -18.81578947368421 Best Reward 0 Last Reward -2 Epsilon 0.4076082248025454\n",
      "Episode 39 Average Reward -18.41025641025641 Best Reward 0 Last Reward -3 Epsilon 0.39993297614052925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j8/qdr4bmbd6vv8nydcr40cfdhw0000gn/T/ipykernel_43203/3307885721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstate_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j8/qdr4bmbd6vv8nydcr40cfdhw0000gn/T/ipykernel_43203/3628056262.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_value_of_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, EPISODES):\n",
    "    state, info = env.reset()\n",
    "    state = np.concatenate((state[0].flatten(), state[1].flatten()))\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        action = agent.choose_action(state, info['mask'])\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.concatenate((state_[0].flatten(), state_[1].flatten()))\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score\n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(\n",
    "                i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            break\n",
    "\n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59669bb1168edc622bb4dae0f982b2741a2a7b3cacb44a90a5e1c2622e59ac77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
